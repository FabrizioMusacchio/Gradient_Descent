{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding gradient descent in machine learning\n",
    "\n",
    "## *Python* code example for fitting two parameters\n",
    "Now we extend the problem by defining a hypothesis function with two parameters, \n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x.$$\n",
    "\n",
    "This time, we visualize $J(\\theta_0,\\theta_1)$  as a contour plot (right panel). The rest follows the same procedure as described for the one parameter case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61498df2cb254e9db19a43e483e6d3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=15, description='N', max=20, min=2), FloatSlider(value=0.7, description=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plots(N, alpha, theta0_true, theta1_true, theta0_start, theta1_start)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "# The data to fit\n",
    "m = 20\n",
    "x = np.linspace(-1, 1, m)\n",
    "y = None\n",
    "\n",
    "def cost_func(x, y, theta0, theta1):\n",
    "    \"\"\"The cost function, J(theta0, theta1) describing the goodness of fit.\"\"\"\n",
    "    return np.average((y - hypothesis(x, theta0, theta1.reshape(-1, 1))) ** 2, axis=1) / 2\n",
    "\n",
    "def hypothesis(x, theta0, theta1):\n",
    "    \"\"\"Our \"hypothesis function\", a straight line.\"\"\"\n",
    "    return theta0.reshape(-1, 1) + theta1 * x\n",
    "\n",
    "def update_plots(N, alpha, theta0_true, theta1_true, theta0_start, theta1_start):\n",
    "    global y\n",
    "\n",
    "    # Generate new y values based on the updated true theta values\n",
    "    y = theta0_true + theta1_true * x\n",
    "\n",
    "    # Create the figure and axes objects\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4.15))\n",
    "\n",
    "    # Initialize the scatter plot for data points\n",
    "    ax[0].scatter(x, y, marker='x', s=40, color='#009E73', label=r\"true $\\theta$\")\n",
    "\n",
    "    # Take N steps with learning rate alpha down the steepest gradient,\n",
    "    # starting at (theta0, theta1) = (theta0_start, theta1_start).\n",
    "    theta = [np.array((theta0_start, theta1_start))]\n",
    "    J = [cost_func(x, y, *theta[0])]\n",
    "    for j in range(N - 1):\n",
    "        last_theta = theta[-1]\n",
    "        this_theta = np.empty((2,))\n",
    "        this_theta[0] = last_theta[0] - alpha / m * np.sum(\n",
    "            (hypothesis(x, *last_theta) - y))\n",
    "        this_theta[1] = last_theta[1] - alpha / m * np.sum(\n",
    "            (hypothesis(x, *last_theta) - y) * x)\n",
    "        theta.append(this_theta)\n",
    "        J.append(cost_func(x, y, *this_theta))\n",
    "\n",
    "    # First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "    # corresponding cost function values.\n",
    "    theta0_grid = np.linspace(-4, 4, 101)\n",
    "    theta1_grid = np.linspace(-5, 5, 101)\n",
    "    X, Y = np.meshgrid(theta0_grid, theta1_grid)\n",
    "    J_grid = cost_func(x, y, X.flatten(), Y.flatten()).reshape(X.shape)\n",
    "\n",
    "    # A pcolor plot for the RHS cost function\n",
    "    pcolor_plot = ax[1].pcolormesh(X, Y, J_grid, cmap='viridis')\n",
    "    fig.colorbar(pcolor_plot, ax=ax[1])\n",
    "\n",
    "    # Contour lines on the pcolor plot\n",
    "    contours = ax[1].contour(X, Y, J_grid, 30, colors='gray')\n",
    "    ax[1].clabel(contours, inline=True, fontsize=8, colors='w')\n",
    "\n",
    "    # The target parameter values indicated on the cost function pcolor plot\n",
    "    ax[1].scatter([theta0_true] * 2, [theta1_true] * 2, s=[50, 10], color=['k', 'w'])\n",
    "\n",
    "    # Annotate the cost function plot with coloured points indicating the\n",
    "    # parameters chosen and red arrows indicating the steps down the gradient.\n",
    "    # Also plot the fit function on the LHS data plot.\n",
    "    for j in range(1, N):\n",
    "        ax[1].annotate('', xy=theta[j], xytext=theta[j - 1],\n",
    "                       arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                       va='center', ha='center')\n",
    "        ax[0].plot(x, hypothesis(x, *theta[j]).flatten(), lw=2, c=\"#0072B2\", alpha=0.75)\n",
    "                   #label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[j])\n",
    "    ax[0].plot(x, hypothesis(x, *theta[j]).flatten(), lw=2, c=\"#E69F00\")\n",
    "    ax[1].scatter(*zip(*theta), s=40, lw=0)\n",
    "    last_theta = theta[j]\n",
    "    ax[1].plot(last_theta[0], last_theta[1], 'o', c=\"orange\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Labels, titles, and a legend.\n",
    "    ax[1].set_xlabel(r'$\\theta_0$')\n",
    "    ax[1].set_ylabel(r'$\\theta_1$')\n",
    "    ax[1].set_title('Cost function')\n",
    "    ax[0].set_xlabel(r'$x$')\n",
    "    ax[0].set_ylabel(r'$y$')\n",
    "    ax[0].set_title('Data and fit')\n",
    "    ax[0].legend(loc='upper left', fontsize='small')\n",
    "    \"\"\" axbox = ax[0].get_position()\n",
    "    # Position the legend by hand so that it doesn't cover up any of the lines.\n",
    "    ax[0].legend(loc=(axbox.x0 + 0.5 * axbox.width, axbox.y0 + 0.1 * axbox.height),\n",
    "                 fontsize='small') \"\"\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders\n",
    "N_slider = IntSlider(min=2, max=20, step=1, value=15)\n",
    "alpha_slider = FloatSlider(min=0.1, max=1.0, step=0.1, value=0.7)\n",
    "theta0_true_slider = FloatSlider(min=-4, max=4, step=0.1, value=2)\n",
    "theta1_true_slider = FloatSlider(min=-4, max=4, step=0.1, value=2)\n",
    "theta0_start_slider = FloatSlider(min=-4, max=4, step=0.1, value=0)\n",
    "theta1_start_slider = FloatSlider(min=-4, max=4, step=0.1, value=0)\n",
    "\n",
    "# Create the interactive widget\n",
    "interact(update_plots,\n",
    "         N=N_slider,\n",
    "         alpha=alpha_slider,\n",
    "         theta0_true=theta0_true_slider,\n",
    "         theta1_true=theta1_true_slider,\n",
    "         theta0_start=theta0_start_slider,\n",
    "         theta1_start=theta1_start_slider)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "The main code is based on the blog post [\"Visualizing the gradient descent method\"](https://scipython.com/blog/visualizing-the-gradient-descent-method/) from [*scipython.com*](https://scipython.com)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradient_descent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
