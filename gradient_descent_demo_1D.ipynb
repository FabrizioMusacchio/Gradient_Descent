{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Gradient Descent in Machine Learning\n",
    "\n",
    "Gradient descent is a fundamental optimization algorithm widely used in machine learning for finding the optimal parameters of a model. It is a powerful technique that enables models to learn from data by iteratively adjusting their parameters to minimize a cost or loss function. In this blog post, we  explore the mathematical background of this method and showcase its implementation in *Python*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematics behind gradient descent\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function of a machine learning model. Its main idea revolves around iteratively adjusting the model's parameters in the direction of the steepest descent of the cost function, aiming to reach the global or local minimum.\n",
    "\n",
    "The mathematical formulation of gradient descent can be represented as follows:\n",
    "\n",
    "The goal is to fit a so-called **hypothesis function** $h_\\theta(x)$ to a given dataset $(x^{(i)}, y^{(i)})$ (with $i=1, 2,\\ldots, m$) by minimizing the **cost function** $J(\\theta)$ in terms of the parameters $\\theta_j$ (with $j=0, 1, \\ldots$). $h_\\theta(x)$ is defined according to the model we want to use. \n",
    "\n",
    "Given a cost function $J(\\theta)$, where $\\theta$ represents the **model's parameters**, we aim to find the values of $\\theta$ that minimize $J(\\theta)$. A common definition of $J(\\theta)$ is the **mean squared error** (MSE) between the predicted values and the actual data, divided by 2,\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_i^m (h_\\theta(x^{(i)} - y^{(i)} ))^2.$$\n",
    "\n",
    "\n",
    "Starting with an initial guess for $\\theta$, the gradient descent algorithm iteratively updates $\\theta$ using the following update rule:\n",
    "\n",
    "$$\\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$$\n",
    "\n",
    "Here, $\\alpha$ denotes the **learning rate**, which controls the step size of each iteration. The learning rate determines how quickly the algorithm converges to the optimal solution, with smaller values leading to slower convergence but potentially more precise results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Python* code example (1D case)\n",
    "\n",
    "Now, let's demonstrate the  implementation of gradient descent using *Python* code. We will need nothing more than the *NumPy* and *Matplotlib* libraries for this purpose. The main code is based on the blog post [\"Visualizing the gradient descent method\"](https://scipython.com/blog/visualizing-the-gradient-descent-method/){{site.data.scuts.extlink}} from [*scipython.com*](https://scipython.com){{site.data.scuts.extlink}}. \n",
    "\n",
    "We define the hypothesis function (`hypothesis function`) as a straight line,\n",
    "\n",
    "$$h_\\theta(x) = \\theta_1 x,$$\n",
    "\n",
    "plotted on the left panel of the figure below. Thus, our problem is 1D and we only fit one parameter $\\theta_1$. The data points are generated from the line $y = 0.5 x$. The cost function $J(\\theta_1)$ (`cost_func(theta1)`) is plotted on the right panel. It visualized how closely the current estimate of $\\theta$ is to the optimal solution, i.e., the  minimum. The cost function is minimized when the predicted values are equal to the actual data, i.e., when the line fits the data perfectly (left panel).\n",
    "\n",
    "\n",
    "The following code is implemented in an interactive *Jupyter* notebook and you can change the main parameters of the algorithm using sliders: \n",
    "\n",
    "* the step size $N$\n",
    "* the learning rate $\\alpha$\n",
    "* the initial parameter value $\\theta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26fde252b514e8eb5b155c039bb4850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='N:', min=1), FloatSlider(value=1.15, description='alphaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# define theta1_true:\n",
    "theta1_true = 0.5\n",
    "\"\"\"\n",
    "theta1_true represents the true or ideal value of the parameter theta1 \n",
    "that we are trying to estimate using the gradient descent algorithm. The \n",
    "goal is to find the optimal value of the parameter theta1 that minimizes \n",
    "the cost function, which measures the goodness of fit between the predicted \n",
    "values and the actual data. By iteratively updating the theta1 value based \n",
    "on the gradient of the cost function, the algorithm attempts to converge \n",
    "to the true or ideal value of theta1 that produces the best fit to the data.\n",
    "\n",
    "In our case, the theta1_true value is set to 0.5 as an example. By comparing the \n",
    "estimated theta1 values obtained from the gradient descent algorithm to the \n",
    "true value, we can assess the accuracy and effectiveness of the algorithm in \n",
    "finding the optimal parameter value.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# The data to fit\n",
    "m = 20\n",
    "x = np.linspace(-1, 1, m)\n",
    "y = theta1_true * x\n",
    "\n",
    "\n",
    "# Define the plot function\n",
    "def update_plot(N, alpha, theta1):\n",
    "    # Clear previous plot\n",
    "    plt.clf()\n",
    "\n",
    "    def cost_func(theta1):\n",
    "        \"\"\"The cost function, J(theta1) describing the goodness of fit.\"\"\"\n",
    "        theta1 = np.atleast_2d(np.asarray(theta1))\n",
    "        return np.average((y - hypothesis(x, theta1))**2, axis=1) / 2\n",
    "\n",
    "    def hypothesis(x, theta1):\n",
    "        \"\"\"Our \"hypothesis function\", a straight line through the origin.\"\"\"\n",
    "        return theta1 * x\n",
    "\n",
    "    # First construct a grid of theta1 parameter pairs and their corresponding\n",
    "    # cost function values.\n",
    "    theta1_grid = np.linspace(-0.2, 1.2, 50)\n",
    "    J_grid = cost_func(theta1_grid[:, np.newaxis])\n",
    "\n",
    "    # The cost function as a function of its single parameter, theta1.\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(x, y, marker='x', s=40, color='#009E73', label=r\"true $\\theta$\")\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$y$')\n",
    "    plt.title('Data and fit')\n",
    "    \n",
    "    # Take N steps with learning rate alpha down the steepest gradient,\n",
    "    # starting at theta1.\n",
    "    theta1_values = [theta1]\n",
    "    J_values = [np.array(cost_func(theta1_values[0])[0])]\n",
    "    for j in range(N - 1):\n",
    "        last_theta1 = theta1_values[-1]\n",
    "        this_theta1 = last_theta1 - alpha / m * np.sum(\n",
    "            (hypothesis(x, last_theta1) - y) * x)\n",
    "        theta1_values.append(this_theta1)\n",
    "        J_values.append(np.array(cost_func(this_theta1)[0]))\n",
    "\n",
    "    # Annotate the cost function plot with coloured points indicating the\n",
    "    # parameters chosen and red arrows indicating the steps down the gradient.\n",
    "    # Also plot the fit function on the LHS data plot.\n",
    "    for j in range(N):\n",
    "        plt.plot(x, hypothesis(x, theta1_values[j]), lw=1.5, c=\"#0072B2\", alpha=0.75)\n",
    "                 #label=r'$\\theta_1 = {:.3f}$'.format(theta1_values[j])\n",
    "    plt.plot(x, hypothesis(x, theta1_values[j]), lw=2, c=\"orange\")\n",
    "    plt.legend(loc='upper left', fontsize='small')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(theta1_grid, J_grid, 'k')\n",
    "    plt.scatter(theta1_values, J_values, s=40, lw=0, c=\"#0072B2\")\n",
    "    plt.plot(theta1_values[-1:], J_values[-1:], 'o', c=\"orange\", label=r\"final $\\theta_1$\")\n",
    "    plt.plot(theta1_true, 0, '.', c=\"#009E73\", label=r\"true $\\theta$\")\n",
    "    #plt.xlim(-0.2, 1.2)\n",
    "    plt.legend(loc='upper right', fontsize='small')\n",
    "    plt.xlabel(r'$\\theta_1$')\n",
    "    plt.ylabel(r'$J(\\theta_1)$')\n",
    "    plt.title('Cost function')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders\n",
    "slider_N = widgets.IntSlider(value=20, min=1, max=100, description='N:')\n",
    "slider_alpha = widgets.FloatSlider(value=1.15, min=0.1, max=2.0, step=0.05, description='alpha:')\n",
    "slider_theta1 = widgets.FloatSlider(value=0.0, min=-0.2, max=1.2, step=0.05, description='theta_1:')\n",
    "\n",
    "# Register the update_plot function as the callback for each slider using interactive\n",
    "interactive_plot = widgets.interactive(update_plot, N=slider_N, alpha=slider_alpha, theta1=slider_theta1)\n",
    "\n",
    "# Display the interactive plot\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "The main code is based on the blog post [\"Visualizing the gradient descent method\"](https://scipython.com/blog/visualizing-the-gradient-descent-method/) from [*scipython.com*](https://scipython.com)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradient_descent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
